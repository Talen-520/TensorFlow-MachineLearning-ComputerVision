本章为如何旋转，倾斜等图片处理方式增加训练模型准确性

当训练数据较少时，可以通过augmentation增加数据训练类型，减少过度拟合overfitting 但如果验证数据与训练数据过度相同，会导致反效果降低准确性

overfitting refers to a phenomenon where a model becomes too specialized to the training data and performs poorly on unseen or new data. It occurs when a model learns to capture the noise or random fluctuations in the training data rather than the underlying patterns or relationships.

Lab 1

In the previous video, we looked at training a small data set of cats versus dogs, and saw how overfitting occurred relatively early on in the training, leading us to a false sense of security about how well the neural network could perform. Let's now take a look at the impact of adding image augmentation to the training. Here we have exactly the same code except that we've added the image augmentation code to it. I'll start the training, and we'll see that we have 2,000 training images in two classes. As we start training, we'll initially see that the accuracy is lower than with the non-augmented version we did earlier. This is because of the random effects of the different image processing that's being done. As it runs for a few more epochs, you'll see the accuracy slowly climbing. I'll skip forward to see the last few epochs, and by the time we reach the last one, our model's about 86 percent accurate on the training data, and about 81 percent on the test data. So let's plot this. We can see that the training and validation accuracy, and loss are actually in step with each other. This is a clear sign that we've solved the overfitting that we had earlier. While our accuracy is a little lower, it's also trending upwards so perhaps a more epochs will get us closer to 100 percent. Why don't you go ahead and give it a try?

Lab 2
Of course, image augmentation isn't the magic bullet to cure overfitting. It really helps to have a massive diversity of images. So for example, if we look at the horses or humans data set and train it for the same epochs, then we can take a look at its behavior. So I'm going to start training and show all 100 epochs. I sped it up a bit to save your time. As you watch, you'll see the test accuracy climbing steadily. At first, the validation accuracy seems to be in step, but then you'll see it varying wildly. What's happening here is that despite the image augmentation, the diversity of images is still too sparse and the validation set may also be poorly designed, namely that the type of image in it is too close to the images in the training set. If you inspect the data for yourself you'll see that's the case. For example, the humans are almost always standing up and in the center of the picture, in both the training and validation sets, so augmenting the image will change it to look like something that doesn't look like what's in the validation set. So by the time the training has completed, we can see the same pattern. The training accuracy is trending towards 100 percent, but the validation is fluctuating in the 60s and 70s. Let's plot this, we can see that the training accuracy climbs steadily in the way that we would want, but the validation fluctuated like crazy. So what we can learn from this is that the image augmentation introduces a random element to the training images but if the validation set doesn't have the same randomness, then its results can fluctuate like this.


So bear in mind that you don't just need a broad set of images for training, you also need them for testing or the image augmentation won't help you very much.

